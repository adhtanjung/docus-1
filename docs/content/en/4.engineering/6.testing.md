---
title: Testing Strategy
description: Test types, coverage expectations, and AI-generated code testing.
---

# Testing Strategy

Automated testing is the primary mechanism for ensuring code correctness over time. This page defines the testing standards for the HAER platform.

## Test Types

### 1. Unit Tests

- **Scope**: Individual functions, classes, or modules in isolation.
- **Speed**: Very fast (< 1 second per test).
- **Dependencies**: Mocked or stubbed.
- **Location**: `*.spec.ts` files alongside the source code.
- **Framework**: Jest.

### 2. Integration Tests

- **Scope**: Interactions between multiple modules (e.g., Service + Database).
- **Speed**: Moderate (may involve DB setup/teardown).
- **Dependencies**: Real databases (test containers or in-memory).
- **Location**: `*.integration.spec.ts` files.
- **Framework**: Jest with `@nestjs/testing`.

### 3. End-to-End (E2E) Tests

- **Scope**: Full application workflows from API request to response.
- **Speed**: Slowest (full server environment).
- **Dependencies**: Running server instance.
- **Location**: `test/e2e/` directory.
- **Framework**: Jest or Playwright (for UI).

## Coverage Expectations

| Metric                        | Target | Minimum |
| :---------------------------- | :----- | :------ |
| **Unit Test Coverage**        | > 90%  | 80%     |
| **Integration Test Coverage** | > 70%  | 60%     |
| **Critical Paths**            | 100%   | 100%    |

### Critical Paths (Must Be Covered)

- Authentication and authorization logic.
- Data mutation endpoints (Create, Update, Delete).
- Payment or financial calculations.
- Audit logging.

## Test Naming Conventions

Use the following pattern for test descriptions:

```text
[Unit Under Test] should [Expected Behavior] when [State/Condition]
```

**Example:**

```typescript
describe('L1OrganizationService', () => {
  describe('create', () => {
    it('should create a new organization when valid data is provided', async () => { ... });
    it('should throw ConflictException when code already exists', async () => { ... });
  });
});
```

## AI-Generated Code Testing

::prose-note
**AI-written logic must have tests. Generated code without tests is considered incomplete.**
::

### Additional Requirements for AI Code

- **Higher Scrutiny**: Review generated tests for correctness; AI can write tests that pass but don't actually test anything.
- **Edge Case Focus**: Explicitly ask the AI to generate tests for edge cases and failure modes.
- **Manual Verification**: Ensure at least one test was written or verified by a human.

## TDD Workflow

The HAER project follows **Test-Driven Development (TDD)** for core logic.

::steps
### 1. RED

Write a failing test that defines the expected behavior.

### 2. GREEN

Write the minimum code required to make the test pass.

### 3. REFACTOR

Improve the code while keeping the test green.
::

## Avoiding Flaky Tests

Flaky tests erode trust in the test suite.

| Anti-Pattern            | Solution                                  |
| :---------------------- | :---------------------------------------- |
| Relying on `setTimeout` | Use proper async/await or test utilities. |
| Shared mutable state    | Isolate test data per test case.          |
| Order-dependent tests   | Ensure each test is independent.          |
| External service calls  | Mock external dependencies.               |

## Mocking Strategy

- **Prefer Fakes Over Mocks**: A simple in-memory implementation is often clearer than complex mock setups.
- **Mock at Boundaries**: Mock external services (HTTP clients, databases) not internal modules.
- **Verify Interactions Sparingly**: Over-verifying mock calls makes tests brittle.
